{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feedparser.util import FeedParserDict\n",
    "import feedparser\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import bibtexparser\n",
    "import numpy as np\n",
    "import pprint\n",
    "import copy\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['bozo', 'entries', 'feed', 'headers', 'encoding', 'version', 'bozo_exception', 'namespaces'])\n"
     ]
    }
   ],
   "source": [
    "#LOAD DATASET\n",
    "\n",
    "dois:FeedParserDict = feedparser.parse(open(\"Arxiv Search.txt\", 'r', encoding='utf-8').read())\n",
    "\n",
    "keys = dois.keys()\n",
    "print(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'arxiv_comment': '14 pages, 6 figures. Published at Springer Neural Computing '\n",
      "                  'and\\n'\n",
      "                  '  Applications journal',\n",
      " 'arxiv_doi': '10.1007/s00521-022-08068-6',\n",
      " 'arxiv_journal_ref': 'Neural Comput & Applic (2022)',\n",
      " 'arxiv_primary_category': {'scheme': 'http://arxiv.org/schemas/atom',\n",
      "                            'term': 'cs.CL'},\n",
      " 'author': 'João Baptista de Oliveira e Souza Filho',\n",
      " 'author_detail': {'name': 'João Baptista de Oliveira e Souza Filho'},\n",
      " 'authors': [{'name': 'Frederico Dias Souza'},\n",
      "             {'name': 'João Baptista de Oliveira e Souza Filho'}],\n",
      " 'guidislink': True,\n",
      " 'id': 'http://arxiv.org/abs/2212.00587v1',\n",
      " 'link': 'http://arxiv.org/abs/2212.00587v1',\n",
      " 'links': [{'href': 'http://dx.doi.org/10.1007/s00521-022-08068-6',\n",
      "            'rel': 'related',\n",
      "            'title': 'doi',\n",
      "            'type': 'text/html'},\n",
      "           {'href': 'http://arxiv.org/abs/2212.00587v1',\n",
      "            'rel': 'alternate',\n",
      "            'type': 'text/html'},\n",
      "           {'href': 'http://arxiv.org/pdf/2212.00587v1',\n",
      "            'rel': 'related',\n",
      "            'title': 'pdf',\n",
      "            'type': 'application/pdf'}],\n",
      " 'published': '2022-12-01T15:24:19Z',\n",
      " 'published_parsed': time.struct_time(tm_year=2022, tm_mon=12, tm_mday=1, tm_hour=15, tm_min=24, tm_sec=19, tm_wday=3, tm_yday=335, tm_isdst=0),\n",
      " 'summary': 'Text classification is a natural language processing (NLP) task '\n",
      "            'relevant to\\n'\n",
      "            'many commercial applications, like e-commerce and customer '\n",
      "            'service. Naturally,\\n'\n",
      "            'classifying such excerpts accurately often represents a '\n",
      "            'challenge, due to\\n'\n",
      "            'intrinsic language aspects, like irony and nuance. To accomplish '\n",
      "            'this task, one\\n'\n",
      "            'must provide a robust numerical representation for documents, a '\n",
      "            'process known\\n'\n",
      "            'as embedding. Embedding represents a key NLP field nowadays, '\n",
      "            'having faced a\\n'\n",
      "            'significant advance in the last decade, especially after the '\n",
      "            'introduction of\\n'\n",
      "            'the word-to-vector concept and the popularization of Deep '\n",
      "            'Learning models for\\n'\n",
      "            'solving NLP tasks, including Convolutional Neural Networks '\n",
      "            '(CNNs), Recurrent\\n'\n",
      "            'Neural Networks (RNNs), and Transformer-based Language Models '\n",
      "            '(TLMs). Despite\\n'\n",
      "            'the impressive achievements in this field, the literature '\n",
      "            'coverage regarding\\n'\n",
      "            'generating embeddings for Brazilian Portuguese texts is scarce, '\n",
      "            'especially when\\n'\n",
      "            'considering commercial user reviews. Therefore, this work aims to '\n",
      "            'provide a\\n'\n",
      "            'comprehensive experimental study of embedding approaches '\n",
      "            'targeting a binary\\n'\n",
      "            'sentiment classification of user reviews in Brazilian Portuguese. '\n",
      "            'This study\\n'\n",
      "            'includes from classical (Bag-of-Words) to state-of-the-art '\n",
      "            '(Transformer-based)\\n'\n",
      "            'NLP models. The methods are evaluated with five open-source '\n",
      "            'databases with\\n'\n",
      "            'pre-defined data partitions made available in an open digital '\n",
      "            'repository to\\n'\n",
      "            'encourage reproducibility. The Fine-tuned TLMs achieved the best '\n",
      "            'results for\\n'\n",
      "            'all cases, being followed by the Feature-based TLM, LSTM, and '\n",
      "            'CNN, with\\n'\n",
      "            'alternate ranks, depending on the database under analysis.',\n",
      " 'summary_detail': {'base': '',\n",
      "                    'language': None,\n",
      "                    'type': 'text/plain',\n",
      "                    'value': 'Text classification is a natural language '\n",
      "                             'processing (NLP) task relevant to\\n'\n",
      "                             'many commercial applications, like e-commerce '\n",
      "                             'and customer service. Naturally,\\n'\n",
      "                             'classifying such excerpts accurately often '\n",
      "                             'represents a challenge, due to\\n'\n",
      "                             'intrinsic language aspects, like irony and '\n",
      "                             'nuance. To accomplish this task, one\\n'\n",
      "                             'must provide a robust numerical representation '\n",
      "                             'for documents, a process known\\n'\n",
      "                             'as embedding. Embedding represents a key NLP '\n",
      "                             'field nowadays, having faced a\\n'\n",
      "                             'significant advance in the last decade, '\n",
      "                             'especially after the introduction of\\n'\n",
      "                             'the word-to-vector concept and the '\n",
      "                             'popularization of Deep Learning models for\\n'\n",
      "                             'solving NLP tasks, including Convolutional '\n",
      "                             'Neural Networks (CNNs), Recurrent\\n'\n",
      "                             'Neural Networks (RNNs), and Transformer-based '\n",
      "                             'Language Models (TLMs). Despite\\n'\n",
      "                             'the impressive achievements in this field, the '\n",
      "                             'literature coverage regarding\\n'\n",
      "                             'generating embeddings for Brazilian Portuguese '\n",
      "                             'texts is scarce, especially when\\n'\n",
      "                             'considering commercial user reviews. Therefore, '\n",
      "                             'this work aims to provide a\\n'\n",
      "                             'comprehensive experimental study of embedding '\n",
      "                             'approaches targeting a binary\\n'\n",
      "                             'sentiment classification of user reviews in '\n",
      "                             'Brazilian Portuguese. This study\\n'\n",
      "                             'includes from classical (Bag-of-Words) to '\n",
      "                             'state-of-the-art (Transformer-based)\\n'\n",
      "                             'NLP models. The methods are evaluated with five '\n",
      "                             'open-source databases with\\n'\n",
      "                             'pre-defined data partitions made available in an '\n",
      "                             'open digital repository to\\n'\n",
      "                             'encourage reproducibility. The Fine-tuned TLMs '\n",
      "                             'achieved the best results for\\n'\n",
      "                             'all cases, being followed by the Feature-based '\n",
      "                             'TLM, LSTM, and CNN, with\\n'\n",
      "                             'alternate ranks, depending on the database under '\n",
      "                             'analysis.'},\n",
      " 'tags': [{'label': None,\n",
      "           'scheme': 'http://arxiv.org/schemas/atom',\n",
      "           'term': 'cs.CL'},\n",
      "          {'label': None,\n",
      "           'scheme': 'http://arxiv.org/schemas/atom',\n",
      "           'term': 'cs.AI'}],\n",
      " 'title': 'Embedding generation for text classification of Brazilian '\n",
      "          'Portuguese\\n'\n",
      "          '  user reviews: from bag-of-words to transformers',\n",
      " 'title_detail': {'base': '',\n",
      "                  'language': None,\n",
      "                  'type': 'text/plain',\n",
      "                  'value': 'Embedding generation for text classification of '\n",
      "                           'Brazilian Portuguese\\n'\n",
      "                           '  user reviews: from bag-of-words to transformers'},\n",
      " 'updated': '2022-12-01T15:24:19Z',\n",
      " 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=12, tm_mday=1, tm_hour=15, tm_min=24, tm_sec=19, tm_wday=3, tm_yday=335, tm_isdst=0)}\n"
     ]
    }
   ],
   "source": [
    "#EXAMPLE\n",
    "pprint.pprint(dois['entries'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE INDEX AND DOI ONLY\n",
    "results2doi = pd.DataFrame(columns=['idx', 'doi'])\n",
    "\n",
    "i = 0    \n",
    "with tqdm(total=len(dois['entries'])) as pbar:\n",
    "    for entry in dois['entries']:\n",
    "        try:                    \n",
    "            link = entry['id']            \n",
    "            doi:str = link[link.rfind('/')+1:]            \n",
    "            results2doi = pd.concat([results2doi, pd.DataFrame([[i, doi]], columns=results2doi.columns)])\n",
    "            pbar.update()\n",
    "            i+=1\n",
    "        except Exception as e:\n",
    "            pass\n",
    "results2doi.to_csv(\"./Arxiv_doi.csv\", index=False, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REMOVE DUPLICATES\n",
    "arxiv_doi = pd.read_csv('./Arxiv_doi.csv')\n",
    "\n",
    "copia = arxiv_doi.copy(deep=True)\n",
    "copia['doi'] = copia['doi'].apply(lambda x: x[:-2])\n",
    "copia.drop_duplicates('doi', inplace=True)\n",
    "\n",
    "arxiv_doi = arxiv_doi.merge(copia, how='inner', on=\"idx\")\n",
    "arxiv_doi.drop('doi_y', axis=1, inplace=True)\n",
    "\n",
    "arxiv_doi['doi'] = arxiv_doi['doi_x']\n",
    "arxiv_doi.drop('doi_x', axis=1, inplace=True)\n",
    "\n",
    "arxiv_doi.to_csv('./no_duplicates_doi.csv', index=False, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GET ITEMS FROM DOI\n",
    "no_dup = pd.read_csv('./no_duplicates_doi.csv')\n",
    "copia = []\n",
    "for i in range(len(dois['entries'])):\n",
    "    if i in no_dup['idx']:\n",
    "        copia.append(dois['entries'][i])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REMOVE PUBLISHED BEFORE 2015\n",
    "rem = []\n",
    "for entry in dois['entries']:\n",
    "    if entry['published_parsed'].tm_year < 2015:\n",
    "        rem+= [entry]\n",
    "for r in rem:\n",
    "    try: \n",
    "        copia.remove(r)\n",
    "    except:\n",
    "        continue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "copia = [json.loads(json.dumps(x)) for x in copia]\n",
    "doi_codes = []\n",
    "for entry in copia:\n",
    "    doi_codes+= [entry['link'][entry['link'].rfind('/') + 1:]]\n",
    "\n",
    "pd.DataFrame(doi_codes).to_csv(\"doi2bib.csv\", index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'http://arxiv.org/abs/2212.00587v1',\n",
       " 'guidislink': True,\n",
       " 'link': 'http://arxiv.org/abs/2212.00587v1',\n",
       " 'updated': '2022-12-01T15:24:19Z',\n",
       " 'updated_parsed': [2022, 12, 1, 15, 24, 19, 3, 335, 0],\n",
       " 'published': '2022-12-01T15:24:19Z',\n",
       " 'published_parsed': [2022, 12, 1, 15, 24, 19, 3, 335, 0],\n",
       " 'title': 'Embedding generation for text classification of Brazilian Portuguese\\n  user reviews: from bag-of-words to transformers',\n",
       " 'title_detail': {'type': 'text/plain',\n",
       "  'language': None,\n",
       "  'base': '',\n",
       "  'value': 'Embedding generation for text classification of Brazilian Portuguese\\n  user reviews: from bag-of-words to transformers'},\n",
       " 'summary': 'Text classification is a natural language processing (NLP) task relevant to\\nmany commercial applications, like e-commerce and customer service. Naturally,\\nclassifying such excerpts accurately often represents a challenge, due to\\nintrinsic language aspects, like irony and nuance. To accomplish this task, one\\nmust provide a robust numerical representation for documents, a process known\\nas embedding. Embedding represents a key NLP field nowadays, having faced a\\nsignificant advance in the last decade, especially after the introduction of\\nthe word-to-vector concept and the popularization of Deep Learning models for\\nsolving NLP tasks, including Convolutional Neural Networks (CNNs), Recurrent\\nNeural Networks (RNNs), and Transformer-based Language Models (TLMs). Despite\\nthe impressive achievements in this field, the literature coverage regarding\\ngenerating embeddings for Brazilian Portuguese texts is scarce, especially when\\nconsidering commercial user reviews. Therefore, this work aims to provide a\\ncomprehensive experimental study of embedding approaches targeting a binary\\nsentiment classification of user reviews in Brazilian Portuguese. This study\\nincludes from classical (Bag-of-Words) to state-of-the-art (Transformer-based)\\nNLP models. The methods are evaluated with five open-source databases with\\npre-defined data partitions made available in an open digital repository to\\nencourage reproducibility. The Fine-tuned TLMs achieved the best results for\\nall cases, being followed by the Feature-based TLM, LSTM, and CNN, with\\nalternate ranks, depending on the database under analysis.',\n",
       " 'summary_detail': {'type': 'text/plain',\n",
       "  'language': None,\n",
       "  'base': '',\n",
       "  'value': 'Text classification is a natural language processing (NLP) task relevant to\\nmany commercial applications, like e-commerce and customer service. Naturally,\\nclassifying such excerpts accurately often represents a challenge, due to\\nintrinsic language aspects, like irony and nuance. To accomplish this task, one\\nmust provide a robust numerical representation for documents, a process known\\nas embedding. Embedding represents a key NLP field nowadays, having faced a\\nsignificant advance in the last decade, especially after the introduction of\\nthe word-to-vector concept and the popularization of Deep Learning models for\\nsolving NLP tasks, including Convolutional Neural Networks (CNNs), Recurrent\\nNeural Networks (RNNs), and Transformer-based Language Models (TLMs). Despite\\nthe impressive achievements in this field, the literature coverage regarding\\ngenerating embeddings for Brazilian Portuguese texts is scarce, especially when\\nconsidering commercial user reviews. Therefore, this work aims to provide a\\ncomprehensive experimental study of embedding approaches targeting a binary\\nsentiment classification of user reviews in Brazilian Portuguese. This study\\nincludes from classical (Bag-of-Words) to state-of-the-art (Transformer-based)\\nNLP models. The methods are evaluated with five open-source databases with\\npre-defined data partitions made available in an open digital repository to\\nencourage reproducibility. The Fine-tuned TLMs achieved the best results for\\nall cases, being followed by the Feature-based TLM, LSTM, and CNN, with\\nalternate ranks, depending on the database under analysis.'},\n",
       " 'authors': [{'name': 'Frederico Dias Souza'},\n",
       "  {'name': 'João Baptista de Oliveira e Souza Filho'}],\n",
       " 'author_detail': {'name': 'João Baptista de Oliveira e Souza Filho'},\n",
       " 'author': 'João Baptista de Oliveira e Souza Filho',\n",
       " 'arxiv_doi': {'xmlns:arxiv': 'http://arxiv.org/schemas/atom'},\n",
       " 'links': [{'title': 'doi',\n",
       "   'href': 'http://dx.doi.org/10.1007/s00521-022-08068-6',\n",
       "   'rel': 'related',\n",
       "   'type': 'text/html'},\n",
       "  {'href': 'http://arxiv.org/abs/2212.00587v1',\n",
       "   'rel': 'alternate',\n",
       "   'type': 'text/html'},\n",
       "  {'title': 'pdf',\n",
       "   'href': 'http://arxiv.org/pdf/2212.00587v1',\n",
       "   'rel': 'related',\n",
       "   'type': 'application/pdf'}],\n",
       " 'arxiv_comment': {'xmlns:arxiv': 'http://arxiv.org/schemas/atom'},\n",
       " 'arxiv_journal_ref': {'xmlns:arxiv': 'http://arxiv.org/schemas/atom'},\n",
       " 'arxiv_primary_category': {'xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "  'term': 'cs.CL',\n",
       "  'scheme': 'http://arxiv.org/schemas/atom'},\n",
       " 'tags': [{'term': 'cs.CL',\n",
       "   'scheme': 'http://arxiv.org/schemas/atom',\n",
       "   'label': None},\n",
       "  {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(json.dumps(copia[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17249\n",
      "14968\n",
      "2281\n",
      "12335\n"
     ]
    }
   ],
   "source": [
    "print(len(no_dup['idx']))\n",
    "print(len(copia))\n",
    "print(len(no_dup['idx']) - len(copia))\n",
    "print(len(rem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bib_database = bibtexparser.load(bibtex_file=open(\"./k2.bib\", 'r'))\n",
    "#df = pd.DataFrame(bib_database.entries)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
